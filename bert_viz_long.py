import re
from transformers import BertTokenizer, BertModel
from bertviz import head_view
import webbrowser
import os
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import json

class PlantUMLBertAttention:
    def __init__(self, model_path: str = 'C:\\AppAndData\\codeAndproject\\bertBaseUncased'):
        self.model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.max_length = 512

    def preprocess_plantuml(self, plantuml_text: str) -> str:
        # ç§»é™¤PlantUMLè¯­æ³•æ ‡è®°
        text = re.sub(r'@startuml|@enduml', '', plantuml_text)
        
        # æ¸…ç†æ ¼å¼
        text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
        text = re.sub(r'[{}]', '', text)  # ç§»é™¤å¤§æ‹¬å·
        text = re.sub(r'[-+#~]', '', text)  # ç§»é™¤å¯è§æ€§ä¿®é¥°ç¬¦
        text = text.strip()
        
        print("å¤„ç†åçš„plantuml:",text)
        return text

    def preprocess_description(self, description: str) -> str:
        # ç§»é™¤markdownæ ¼å¼
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', description)  # ç§»é™¤ç²—ä½“
        text = re.sub(r'`([^`]+)`', r'\1', description)  # ç§»é™¤ä»£ç æ ‡è®°
        text = re.sub(r'#+\s*', '', text)  # ç§»é™¤æ ‡é¢˜æ ‡è®°
        text = re.sub(r'\s+', ' ', text).strip()  # åˆå¹¶ç©ºæ ¼
        
        return text

    def extract_key_plantuml_elements(self, plantuml_text: str, max_tokens: int = 200) -> str:
        """
        ä»PlantUMLä¸­æå–å…³é”®å…ƒç´ ï¼ˆç±»åã€æ–¹æ³•åç­‰ï¼‰
        """
        # é¢„å¤„ç†
        clean_text = self.preprocess_plantuml(plantuml_text)
        
        # æå–å…³é”®å…ƒç´ 
        key_elements = []
        
        # æå–ç±»å
        class_pattern = r'class\s+(\w+)'
        classes = re.findall(class_pattern, clean_text, re.IGNORECASE)
        key_elements.extend([f"class {cls}" for cls in classes])
        
        # æå–æ¥å£å
        interface_pattern = r'interface\s+(\w+)'
        interfaces = re.findall(interface_pattern, clean_text, re.IGNORECASE)
        key_elements.extend([f"interface {iface}" for iface in interfaces])
        
        # æå–æ–¹æ³•å
        method_pattern = r'(\w+)\s*\([^)]*\)\s*:\s*(\w+)'
        methods = re.findall(method_pattern, clean_text)
        key_elements.extend([f"{method} {return_type}" for method, return_type in methods])
        
        # æå–å±æ€§
        attribute_pattern = r'(\w+)\s*:\s*(\w+)'
        attributes = re.findall(attribute_pattern, clean_text)
        key_elements.extend([f"{attr} {attr_type}" for attr, attr_type in attributes])
        
        # æå–å…³ç³»
        relation_patterns = [
            r'(\w+)\s+extends\s+(\w+)',
            r'(\w+)\s+implements\s+(\w+)',
            r'(\w+)\s*-->\s*(\w+)',
            r'(\w+)\s*--\s*(\w+)'
        ]
        
        for pattern in relation_patterns:
            relations = re.findall(pattern, clean_text, re.IGNORECASE)
            key_elements.extend([f"{rel[0]} relates {rel[1]}" for rel in relations])
        
        # ç»„åˆå…³é”®å…ƒç´ 
        key_text = ' '.join(key_elements)
        
        # æ£€æŸ¥é•¿åº¦å¹¶æˆªæ–­
        tokens = self.tokenizer.tokenize(key_text)
        if len(tokens) > max_tokens:
            tokens = tokens[:max_tokens]
            key_text = self.tokenizer.convert_tokens_to_string(tokens)
        
        return key_text

    def extract_key_description_elements(self, description: str, max_tokens: int = 200) -> str:
        """
        ä»ç³»ç»Ÿæè¿°ä¸­æå–å…³é”®ä¿¡æ¯
        """
        clean_desc = self.preprocess_description(description)
        
        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'[.!?]+', clean_desc)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # æå–åŒ…å«å…³é”®è¯çš„å¥å­
        key_words = ['class', 'interface', 'system', 'device', 'method', 'function', 
                    'attribute', 'property', 'extends', 'implements', 'inherit']
        
        key_sentences = []
        for sentence in sentences:
            if any(keyword in sentence.lower() for keyword in key_words):
                key_sentences.append(sentence)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…³é”®å¥å­ï¼Œä½¿ç”¨å‰å‡ å¥
        if not key_sentences:
            key_sentences = sentences[:3]
        
        key_text = ' '.join(key_sentences)
        
        # æ£€æŸ¥é•¿åº¦å¹¶æˆªæ–­
        tokens = self.tokenizer.tokenize(key_text)
        if len(tokens) > max_tokens:
            tokens = tokens[:max_tokens]
            key_text = self.tokenizer.convert_tokens_to_string(tokens)
        
        return key_text

    def check_input_length(self, text1: str, text2: str) -> Tuple[bool, Dict]:
        """
        æ£€æŸ¥è¾“å…¥æ–‡æœ¬çš„é•¿åº¦æ˜¯å¦é€‚åˆBERTå¤„ç†
        """
        tokens1 = self.tokenizer.tokenize(text1)
        tokens2 = self.tokenizer.tokenize(text2)
        
        # è€ƒè™‘ç‰¹æ®Štoken: [CLS], [SEP], [SEP]
        total_tokens = len(tokens1) + len(tokens2) + 3
        
        return total_tokens <= self.max_length, {
            'text1_tokens': len(tokens1),
            'text2_tokens': len(tokens2),
            'total_tokens': total_tokens,
            'max_length': self.max_length,
            'is_truncated': total_tokens > self.max_length
        }

    def create_lightweight_visualization(self, sentence_a: str, sentence_b: str, 
                                       max_tokens_total: int = 200,
                                       selected_layers: List[int] = [2, 5, 10],
                                       selected_heads: List[int] = [0, 5, 6, 10]):
        """
        åˆ›å»ºè½»é‡çº§å¯è§†åŒ–ï¼Œé¿å…HTMLæ–‡ä»¶è¿‡å¤§
        """
        print("åˆ›å»ºè½»é‡çº§å¯è§†åŒ–...")
        
        # ç¡®ä¿è¾“å…¥æ–‡æœ¬ä¸ä¼šå¤ªé•¿
        tokens_a = self.tokenizer.tokenize(sentence_a)
        tokens_b = self.tokenizer.tokenize(sentence_b)
        
        # æ ¹æ®æ€»tokené™åˆ¶è°ƒæ•´æ–‡æœ¬é•¿åº¦
        max_a = max_tokens_total // 2
        max_b = max_tokens_total // 2
        
        if len(tokens_a) > max_a:
            tokens_a = tokens_a[:max_a]
            sentence_a = self.tokenizer.convert_tokens_to_string(tokens_a)
        
        if len(tokens_b) > max_b:
            tokens_b = tokens_b[:max_b]
            sentence_b = self.tokenizer.convert_tokens_to_string(tokens_b)
        
        print(f"è°ƒæ•´åçš„æ–‡æœ¬é•¿åº¦: A={len(tokens_a)}, B={len(tokens_b)}")
        
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer.encode_plus(
            sentence_a, 
            sentence_b, 
            return_tensors='pt',
            max_length=max_tokens_total + 3,  # åŠ ä¸Šç‰¹æ®Štoken
            truncation=True,
            padding=True
        )
        
        input_ids = inputs['input_ids']
        token_type_ids = inputs['token_type_ids']

        # è·å–æ¨¡å‹è¾“å‡º
        with torch.no_grad():
            outputs = self.model(input_ids, token_type_ids=token_type_ids)
            attention = outputs[-1]

        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])
        
        # åªé€‰æ‹©éƒ¨åˆ†å±‚å’Œå¤´è¿›è¡Œå¯è§†åŒ–
        filtered_attention = []
        for layer_idx in selected_layers:
            if layer_idx < len(attention):
                layer_attention = attention[layer_idx]
                # åªé€‰æ‹©éƒ¨åˆ†å¤´
                selected_layer_attention = layer_attention[:, selected_heads, :, :]
                filtered_attention.append(selected_layer_attention)
        
        # ç”Ÿæˆè½»é‡çº§å¯è§†åŒ–
        try:
            html_obj = head_view(filtered_attention, tokens, 
                               sentence_b_start=token_type_ids[0].tolist().index(1),
                               html_action='return')
            
            # å†™å…¥è¾ƒå°çš„HTMLæ–‡ä»¶
            filename = "lightweight_attention.html"
            with open(filename, "w", encoding="utf-8") as f:
                f.write(html_obj.data)

            print(f"è½»é‡çº§å¯è§†åŒ–å·²ç”Ÿæˆ: {filename}")
            print(f"åªæ˜¾ç¤ºå±‚: {selected_layers}, å¤´: {selected_heads}")
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(filename) / (1024 * 1024)  # MB
            print(f"æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
            
            if file_size < 10:  # å°äº10MBæ‰è‡ªåŠ¨æ‰“å¼€
                webbrowser.open('file://' + os.path.realpath(filename))
            else:
                print("æ–‡ä»¶ä»ç„¶è¾ƒå¤§ï¼Œè¯·æ‰‹åŠ¨æ‰“å¼€")
                
        except Exception as e:
            print(f"è½»é‡çº§å¯è§†åŒ–ç”Ÿæˆå‡ºé”™: {e}")
            self.create_static_visualization(sentence_a, sentence_b)

    def create_static_visualization(self, sentence_a: str, sentence_b: str,
                                  layer_idx: int = 5,
                                  head_idx: int = 5,
                                  max_tokens_total: int = 100):
        """
        åˆ›å»ºé™æ€matplotlibå¯è§†åŒ–ï¼Œé¿å…äº¤äº’å¼HTMLçš„é—®é¢˜
        """
        print("åˆ›å»ºé™æ€å¯è§†åŒ–...")
        
        # é™åˆ¶æ–‡æœ¬é•¿åº¦
        tokens_a = self.tokenizer.tokenize(sentence_a)[:max_tokens_total//2]
        tokens_b = self.tokenizer.tokenize(sentence_b)[:max_tokens_total//2]
        
        sentence_a = self.tokenizer.convert_tokens_to_string(tokens_a)
        sentence_b = self.tokenizer.convert_tokens_to_string(tokens_b)
        
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer.encode_plus(
            sentence_a, 
            sentence_b, 
            return_tensors='pt',
            max_length=max_tokens_total + 3,
            truncation=True,
            padding=True
        )
        
        input_ids = inputs['input_ids']
        token_type_ids = inputs['token_type_ids']

        # è·å–æ¨¡å‹è¾“å‡º
        with torch.no_grad():
            outputs = self.model(input_ids, token_type_ids=token_type_ids)
            attention = outputs[-1]

        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])
        
        # è·å–ç‰¹å®šå±‚å’Œå¤´çš„æ³¨æ„åŠ›
        layer_attention = attention[layer_idx][0, head_idx].numpy()
        
        # åªæ˜¾ç¤ºå®é™…çš„tokenï¼ˆå»æ‰paddingï¼‰
        actual_length = len([t for t in tokens if t != '[PAD]'])
        layer_attention = layer_attention[:actual_length, :actual_length]
        tokens = tokens[:actual_length]
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(12, 10))
        sns.heatmap(layer_attention, 
                   xticklabels=tokens, 
                   yticklabels=tokens,
                   cmap='Blues',
                   cbar=True)
        
        plt.title(f'BERT Attention Heatmap (Layer {layer_idx}, Head {head_idx})')
        plt.xlabel('Target Tokens')
        plt.ylabel('Source Tokens')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        filename = f"attention_static_L{layer_idx}_H{head_idx}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"é™æ€å¯è§†åŒ–å·²ä¿å­˜: {filename}")

    def create_summary_visualization(self, sentence_a: str, sentence_b: str,
                                   max_tokens_total: int = 150):
        """
        åˆ›å»ºæ³¨æ„åŠ›æ‘˜è¦å¯è§†åŒ–ï¼Œæ˜¾ç¤ºå…³é”®ç»Ÿè®¡ä¿¡æ¯
        """
        print("åˆ›å»ºæ³¨æ„åŠ›æ‘˜è¦...")
        
        # é™åˆ¶æ–‡æœ¬é•¿åº¦
        tokens_a = self.tokenizer.tokenize(sentence_a)[:max_tokens_total//2]
        tokens_b = self.tokenizer.tokenize(sentence_b)[:max_tokens_total//2]
        
        sentence_a = self.tokenizer.convert_tokens_to_string(tokens_a)
        sentence_b = self.tokenizer.convert_tokens_to_string(tokens_b)
        
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer.encode_plus(
            sentence_a, 
            sentence_b, 
            return_tensors='pt',
            max_length=max_tokens_total + 3,
            truncation=True,
            padding=True
        )
        
        input_ids = inputs['input_ids']
        token_type_ids = inputs['token_type_ids']

        # è·å–æ¨¡å‹è¾“å‡º
        with torch.no_grad():
            outputs = self.model(input_ids, token_type_ids=token_type_ids)
            attention = outputs[-1]

        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])
        actual_length = len([t for t in tokens if t != '[PAD]'])
        tokens = tokens[:actual_length]
        
        # åˆ†æä¸åŒå±‚çš„æ³¨æ„åŠ›æ¨¡å¼
        layer_stats = []
        for layer_idx in range(len(attention)):
            layer_att = attention[layer_idx][0].numpy()[:actual_length, :actual_length]
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            avg_attention = np.mean(layer_att)
            max_attention = np.max(layer_att)
            attention_entropy = -np.sum(layer_att * np.log(layer_att + 1e-10), axis=1).mean()
            
            layer_stats.append({
                'layer': layer_idx,
                'avg_attention': avg_attention,
                'max_attention': max_attention,
                'entropy': attention_entropy
            })
        
        # åˆ›å»ºç»Ÿè®¡å›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. å¹³å‡æ³¨æ„åŠ›æƒé‡
        layers = [s['layer'] for s in layer_stats]
        avg_att = [s['avg_attention'] for s in layer_stats]
        axes[0, 0].plot(layers, avg_att, 'b-o')
        axes[0, 0].set_title('Average Attention by Layer')
        axes[0, 0].set_xlabel('Layer')
        axes[0, 0].set_ylabel('Average Attention')
        
        # 2. æœ€å¤§æ³¨æ„åŠ›æƒé‡
        max_att = [s['max_attention'] for s in layer_stats]
        axes[0, 1].plot(layers, max_att, 'r-o')
        axes[0, 1].set_title('Maximum Attention by Layer')
        axes[0, 1].set_xlabel('Layer')
        axes[0, 1].set_ylabel('Maximum Attention')
        
        # 3. æ³¨æ„åŠ›ç†µ
        entropy = [s['entropy'] for s in layer_stats]
        axes[1, 0].plot(layers, entropy, 'g-o')
        axes[1, 0].set_title('Attention Entropy by Layer')
        axes[1, 0].set_xlabel('Layer')
        axes[1, 0].set_ylabel('Entropy')
        
        # 4. æœ€åä¸€å±‚çš„æ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼ˆé™é‡‡æ ·ï¼‰
        last_layer_att = attention[-1][0].mean(dim=0).numpy()[:actual_length, :actual_length]
        
        # å¦‚æœtokenå¤ªå¤šï¼Œè¿›è¡Œé™é‡‡æ ·
        if actual_length > 20:
            step = actual_length // 20
            sampled_att = last_layer_att[::step, ::step]
            sampled_tokens = tokens[::step]
        else:
            sampled_att = last_layer_att
            sampled_tokens = tokens
        
        im = axes[1, 1].imshow(sampled_att, cmap='Blues')
        axes[1, 1].set_title('Last Layer Attention (Averaged)')
        axes[1, 1].set_xticks(range(len(sampled_tokens)))
        axes[1, 1].set_yticks(range(len(sampled_tokens)))
        axes[1, 1].set_xticklabels(sampled_tokens, rotation=45, ha='right')
        axes[1, 1].set_yticklabels(sampled_tokens)
        plt.colorbar(im, ax=axes[1, 1])
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        filename = "attention_summary.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"æ³¨æ„åŠ›æ‘˜è¦å·²ä¿å­˜: {filename}")
        
        # ä¿å­˜æ•°æ®åˆ°JSON
        with open("attention_stats.json", "w") as f:
            json.dump(layer_stats, f, indent=2)
        print("æ³¨æ„åŠ›ç»Ÿè®¡æ•°æ®å·²ä¿å­˜: attention_stats.json")

    def visualize_attention_optimized(self, sentence_a: str, sentence_b: str,
                                    method: str = "lightweight"):
        """
        ä¼˜åŒ–çš„æ³¨æ„åŠ›å¯è§†åŒ–æ–¹æ³•
        
        Args:
            sentence_a: ç¬¬ä¸€ä¸ªå¥å­
            sentence_b: ç¬¬äºŒä¸ªå¥å­  
            method: å¯è§†åŒ–æ–¹æ³• ("lightweight", "static", "summary")
        """
        print(f"ä½¿ç”¨{method}æ–¹æ³•è¿›è¡Œå¯è§†åŒ–...")
        
        # æ£€æŸ¥è¾“å…¥é•¿åº¦
        is_suitable, length_info = self.check_input_length(sentence_a, sentence_b)
        
        if not is_suitable:
            print(f"è¾“å…¥æ–‡æœ¬è¿‡é•¿ ({length_info['total_tokens']} tokens)")
            print("è‡ªåŠ¨æå–å…³é”®ä¿¡æ¯...")
            
            # è‡ªåŠ¨æå–å…³é”®ä¿¡æ¯
            if '@startuml' in sentence_a.lower() or 'class ' in sentence_a.lower():
                sentence_a = self.extract_key_plantuml_elements(sentence_a, 80)
                sentence_b = self.extract_key_description_elements(sentence_b, 80)
            else:
                sentence_b = self.extract_key_plantuml_elements(sentence_b, 80)
                sentence_a = self.extract_key_description_elements(sentence_a, 80)
        
        # æ ¹æ®æ–¹æ³•é€‰æ‹©å¯è§†åŒ–
        if method == "lightweight":
            self.create_lightweight_visualization(sentence_a, sentence_b)
        elif method == "static":
            self.create_static_visualization(sentence_a, sentence_b)
        elif method == "summary":
            self.create_summary_visualization(sentence_a, sentence_b)
        else:
            print(f"æœªçŸ¥æ–¹æ³•: {method}")
            print("ä½¿ç”¨è½»é‡çº§æ–¹æ³•...")
            self.create_lightweight_visualization(sentence_a, sentence_b)

    def visualize_attention_head_view(self, sentence_a: str, sentence_b: str, 
                                    auto_extract_key: bool = True,
                                    max_tokens_per_text: int = 200):
        """
        æ”¹è¿›çš„æ³¨æ„åŠ›å¯è§†åŒ–æ–¹æ³•ï¼Œæ”¯æŒé•¿æ–‡æœ¬å¤„ç†
        """
        # æ£€æŸ¥è¾“å…¥é•¿åº¦
        is_suitable, length_info = self.check_input_length(sentence_a, sentence_b)
        
        if not is_suitable:
            print(f"è­¦å‘Š: è¾“å…¥æ–‡æœ¬è¿‡é•¿ ({length_info['total_tokens']} > {self.max_length} tokens)")
            print("å»ºè®®ä½¿ç”¨ä¼˜åŒ–çš„å¯è§†åŒ–æ–¹æ³•...")
            
            # æä¾›é€‰æ‹©
            print("å¯é€‰æ–¹æ³•:")
            print("1. lightweight - è½»é‡çº§äº¤äº’å¼å¯è§†åŒ–")
            print("2. static - é™æ€å›¾ç‰‡å¯è§†åŒ–")
            print("3. summary - æ³¨æ„åŠ›æ‘˜è¦å¯è§†åŒ–")
            
            # é»˜è®¤ä½¿ç”¨è½»é‡çº§æ–¹æ³•
            self.visualize_attention_optimized(sentence_a, sentence_b, "lightweight")
            return
            
        # åŸæœ‰çš„æ–¹æ³•ç»§ç»­å¤„ç†çŸ­æ–‡æœ¬
        # ... existing code ...

    def visualize_long_text_sliding_window(self, plantuml_text: str, description: str, 
                                         window_size: int = 400, overlap: int = 100):
        """
        ä½¿ç”¨æ»‘åŠ¨çª—å£æ–¹æ³•å¤„ç†é•¿æ–‡æœ¬
        """
        # é¢„å¤„ç†
        clean_plantuml = self.preprocess_plantuml(plantuml_text)
        clean_description = self.preprocess_description(description)
        
        # åˆ†è¯
        plantuml_tokens = self.tokenizer.tokenize(clean_plantuml)
        desc_tokens = self.tokenizer.tokenize(clean_description)
        
        print(f"PlantUML tokens: {len(plantuml_tokens)}")
        print(f"Description tokens: {len(desc_tokens)}")
        
        # å¦‚æœPlantUMLå¤ªé•¿ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£
        if len(plantuml_tokens) > window_size:
            print("PlantUMLè¿‡é•¿ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£å¤„ç†...")
            
            windows = []
            for i in range(0, len(plantuml_tokens), window_size - overlap):
                window = plantuml_tokens[i:i + window_size]
                windows.append(self.tokenizer.convert_tokens_to_string(window))
                
                if i + window_size >= len(plantuml_tokens):
                    break
            
            # ä¸ºæ¯ä¸ªçª—å£ç”Ÿæˆå¯è§†åŒ–
            for idx, window in enumerate(windows):
                print(f"\nç”Ÿæˆç¬¬{idx+1}ä¸ªçª—å£çš„å¯è§†åŒ–...")
                
                # æˆªæ–­æè¿°ä»¥é€‚åº”çª—å£
                desc_for_window = self.extract_key_description_elements(description, 200)
                
                # ä½¿ç”¨ä¼˜åŒ–çš„å¯è§†åŒ–æ–¹æ³•
                self.visualize_attention_optimized(window, desc_for_window, "static")
        else:
            # æ–‡æœ¬ä¸é•¿ï¼Œç›´æ¥å¤„ç†
            self.visualize_attention_optimized(clean_plantuml, clean_description, "lightweight")

# ... existing code ...

def demo_long_text_usage():
    """æ¼”ç¤ºå¦‚ä½•å¤„ç†é•¿PlantUMLå’Œæè¿°æ–‡æœ¬"""
    
    # é•¿PlantUMLç¤ºä¾‹
    long_plantuml = """
    The IndividualBooking class, which implements the IBooking interface, includes attributes such as bookingId (a String unique identifier), checkInDate (a Date for the guest's arrival), and checkOutDate (a Date for the guest's departure), along with methods: calculateTotalCost() that returns a double representing the total booking cost, confirmBooking() that returns a boolean indicating successful booking confirmation, and cancelBooking() that returns a boolean indicating successful booking cancellation.
    """

    long_description = """
    The system facilitates room reservations for individual travelers, groups, and conference events. 
    It captures guest details (e.g., names, contact information) during booking creation.  
    Individual bookings require check-in/check-out dates and accommodate one or more guests in a single reserved room. 
    The system generates a unique confirmation number and sends notifications via the guestâ€™s preferred contact method. 
    
    """

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = PlantUMLBertAttention('bert-base-uncased')

    print("=== ä¼˜åŒ–çš„é•¿æ–‡æœ¬å¤„ç†æ¼”ç¤º ===")
    print("è§£å†³HTMLæ–‡ä»¶è¿‡å¤§é—®é¢˜çš„ä¸‰ç§æ–¹æ³•:")
    
    # æ–¹æ³•1: è½»é‡çº§äº¤äº’å¼å¯è§†åŒ–
    print("\nğŸ”¹ æ–¹æ³•1: è½»é‡çº§äº¤äº’å¼å¯è§†åŒ– (æ¨è)")
    print("ç‰¹ç‚¹: å‡å°‘å±‚æ•°å’Œå¤´æ•°ï¼Œæ–‡ä»¶è¾ƒå°ï¼Œä»å¯äº¤äº’")
    analyzer.visualize_attention_optimized(long_plantuml, long_description, "lightweight")
    
    # æ–¹æ³•2: é™æ€å›¾ç‰‡å¯è§†åŒ–
    print("\nğŸ”¹ æ–¹æ³•2: é™æ€å›¾ç‰‡å¯è§†åŒ–")
    print("ç‰¹ç‚¹: ç”ŸæˆPNGå›¾ç‰‡ï¼Œæ–‡ä»¶å°ï¼Œä½†ä¸å¯äº¤äº’")
    analyzer.visualize_attention_optimized(long_plantuml, long_description, "static")
    
    # æ–¹æ³•3: æ³¨æ„åŠ›æ‘˜è¦å¯è§†åŒ–
    print("\nğŸ”¹ æ–¹æ³•3: æ³¨æ„åŠ›æ‘˜è¦å¯è§†åŒ–")
    print("ç‰¹ç‚¹: æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯å’Œè¶‹åŠ¿ï¼Œæœ€é€‚åˆåˆ†æ")
    analyzer.visualize_attention_optimized(long_plantuml, long_description, "summary")
    
    # æ–¹æ³•4: æ‰‹åŠ¨æ§åˆ¶æ–‡æœ¬é•¿åº¦
    print("\nğŸ”¹ æ–¹æ³•4: æ‰‹åŠ¨æ§åˆ¶æ–‡æœ¬é•¿åº¦")
    print("ç‰¹ç‚¹: ç”¨æˆ·å¯ä»¥ç²¾ç¡®æ§åˆ¶è¦å¯è§†åŒ–çš„å†…å®¹")
    
    # æ‰‹åŠ¨æå–å…³é”®ä¿¡æ¯
    key_plantuml = analyzer.extract_key_plantuml_elements(long_plantuml, max_tokens=80)
    key_description = analyzer.extract_key_description_elements(long_description, max_tokens=80)
    
    print(f"æå–çš„å…³é”®PlantUML: {key_plantuml[:150]}...")
    print(f"æå–çš„å…³é”®æè¿°: {key_description[:150]}...")
    
    # ä½¿ç”¨æå–çš„å…³é”®ä¿¡æ¯è¿›è¡Œå¯è§†åŒ–
    analyzer.create_lightweight_visualization(
        key_plantuml, 
        key_description, 
        max_tokens_total=180,
        selected_layers=[6, 10],  # åªé€‰æ‹©2å±‚
        selected_heads=[0, 1]     # åªé€‰æ‹©2ä¸ªå¤´
    )
    
    print("\n=== ä½¿ç”¨å»ºè®® ===")
    print("1. å¯¹äºé•¿æ–‡æœ¬ï¼Œä¼˜å…ˆä½¿ç”¨ 'lightweight' æ–¹æ³•")
    print("2. å¦‚æœè½»é‡çº§ä»ç„¶è¿‡å¤§ï¼Œä½¿ç”¨ 'static' æ–¹æ³•")
    print("3. å¦‚æœéœ€è¦åˆ†ææ•´ä½“è¶‹åŠ¿ï¼Œä½¿ç”¨ 'summary' æ–¹æ³•")
    print("4. å¯ä»¥é€šè¿‡è°ƒæ•´ max_tokens_total å‚æ•°æ§åˆ¶æ–‡ä»¶å¤§å°")
    print("5. å‡å°‘ selected_layers å’Œ selected_heads å¯ä»¥è¿›ä¸€æ­¥å‡å°æ–‡ä»¶")

def demo_optimized_methods():
    """æ¼”ç¤ºå„ç§ä¼˜åŒ–æ–¹æ³•çš„ä½¿ç”¨"""
    
    print("=== ä¼˜åŒ–æ–¹æ³•å¯¹æ¯”æ¼”ç¤º ===")
    
    # ä¸­ç­‰é•¿åº¦çš„ç¤ºä¾‹
    medium_plantuml = """
    @startuml
    class User {
        + userId: String
        + username: String
        + email: String
        + login(): boolean
        + logout(): void
        + updateProfile(): void
    }
    
    class Order {
        + orderId: String
        + userId: String
        + orderDate: Date
        + totalAmount: float
        + status: String
        + createOrder(): boolean
        + updateStatus(): void
        + calculateTotal(): float
    }
    
    class Product {
        + productId: String
        + name: String
        + price: float
        + category: String
        + stock: int
        + updateStock(): void
        + getPrice(): float
    }
    
    User ||--o{ Order : places
    Order ||--o{ Product : contains
    @enduml
    """
    
    medium_description = """
    This e-commerce system manages users, orders, and products. Users can place multiple orders, 
    and each order can contain multiple products. The system tracks user information, order details, 
    and product inventory. Users authenticate through login/logout functionality and can update their profiles.
    Orders have lifecycle management with status tracking and total calculation. Products maintain 
    inventory levels and pricing information.
    """
    
    analyzer = PlantUMLBertAttention('bert-base-uncased')
    
    print("æ–‡æœ¬é•¿åº¦é€‚ä¸­çš„æƒ…å†µ:")
    is_suitable, length_info = analyzer.check_input_length(medium_plantuml, medium_description)
    print(f"æ€»tokenæ•°: {length_info['total_tokens']}")
    print(f"æ˜¯å¦é€‚åˆç›´æ¥å¤„ç†: {is_suitable}")
    
    if is_suitable:
        print("\nâœ… å¯ä»¥ä½¿ç”¨æ ‡å‡†æ–¹æ³•")
        analyzer.visualize_attention_head_view(medium_plantuml, medium_description, auto_extract_key=False)
    else:
        print("\nâš ï¸ éœ€è¦ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•")
        analyzer.visualize_attention_optimized(medium_plantuml, medium_description, "lightweight")

if __name__ == "__main__":
    print("=== PlantUML BERT æ³¨æ„åŠ›å¯è§†åŒ–å·¥å…· ===")
    print("è§£å†³HTMLæ–‡ä»¶è¿‡å¤§é—®é¢˜çš„ä¼˜åŒ–ç‰ˆæœ¬\n")
    
    # è®©ç”¨æˆ·é€‰æ‹©æ¼”ç¤ºç±»å‹
    while True:
        print("è¯·é€‰æ‹©æ¼”ç¤ºç±»å‹:")
        print("1. é•¿æ–‡æœ¬ä¼˜åŒ–å¤„ç†æ¼”ç¤º")
        print("2. ä¸­ç­‰é•¿åº¦æ–‡æœ¬å¯¹æ¯”æ¼”ç¤º")
        print("3. é€€å‡º")
        
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()
        
        if choice == "1":
            demo_long_text_usage()
            break
        elif choice == "2":
            demo_optimized_methods()
            break
        elif choice == "3":
            print("é€€å‡ºç¨‹åº")
            break
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")